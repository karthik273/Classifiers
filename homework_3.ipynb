{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2017\n",
    "\n",
    "\n",
    "# Homework 3:  Classification Cook-off: Naive Bayes vs Rocchio (plus a little bit of recommenders)\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Wednesday, March 29 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* Hands-on practice building and evaluating classifiers.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as YOUR_UIN_hw3.ipynb. Submit this notebook via eCampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Saturday April 1 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Yelp review data\n",
    "\n",
    "In this assignment, given a Yelp review, your task is to implement two classifiers to predict if the business category of this review is \"food-relevant\" or not, **only based on the review text**. The data is from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge).\n",
    "\n",
    "## Build the training data\n",
    "\n",
    "First, you will need to download this data file as your training data: [training_data.json](https://drive.google.com/open?id=0B_13wIEAmbQMdzBVTndwenoxQlk) \n",
    "\n",
    "The training data file includes 40,000 Yelp reviews. Each line is a json-encoded review, and **you should only focus on the \"text\" field**. As same as in homework 1, you should tokenize the review text by using the regular expression \"\\W+\" (we discussed it in [this Piazza post](https://piazza.com/class/ixkk1fy863r1vs?cid=29). Do NOT remove stop words. **Do casefolding but no stemming**.\n",
    "\n",
    "The label (class) information of each review is in the \"label\" field. It is **either \"Food-relevant\" or \"Food-irrelevant\"**.\n",
    "\n",
    "## Testing data\n",
    "\n",
    "We provide 100 yelp reviews here: [testing_data.json](https://drive.google.com/open?id=0B_13wIEAmbQMbXdyTkhrZDN4Wms). The testing data file has the same format as the training data file. Again, you can get the label informaiton in the \"label\" field. Only use it when you evalute your classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Naive Bayes classifier [35 points]\n",
    "\n",
    "In this part, you will implement a Naive Bayes classifier, which outputs the probabilities that a given review belongs to each class.\n",
    "\n",
    "Use a mixture model that mixes the probability from the document with the general collection frequency of the word. **You should use lambda = 0.7**. Be careful about the decimal rounding since multiplying many probabilities can generate a tiny value. We will not grade on the exact probability value, so feel free to change to logorithm summation (it's not required, though). If the tie case happens, **always go to the \"Food-irrelevant\" side**.\n",
    "\n",
    "### What to report\n",
    "\n",
    "* For the entire testing dataset, report the overall accuracy.\n",
    "* For the class \"Food-relevant\", report the precision and recall.\n",
    "* For the class \"Food-irrelevant\", report the precision and recall.\n",
    "\n",
    "We will also grade on the quality of your code. So make sure that your code is clear and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the naive bayes classifier\n",
    "# Insert as many cells as you want\n",
    "\n",
    "import os\n",
    "import re \n",
    "import json\n",
    "import math\n",
    "user_input = \"/home/karthik/Downloads/training_data.json\"\n",
    "data=[]    #data is copied from json file into this list\n",
    "f = open(user_input,'r')\n",
    "for line in f:\n",
    "    data.append(json.loads(line))\n",
    "fi=0.0     #contains the number of food-irrelevant reviews in training data\n",
    "fr=0.0     #contains the number of food-relevant reviews in training data\n",
    "size_fi = 0.0 #contains the number of tokens in food-irrelevant class in training data including repeated tokens\n",
    "size_fr = 0.0 #contains the number of tokens in food-relevant class in training data including repeated tokens\n",
    "words_in_fr={} #dictionary to story unique tokens in food-relevant class in training data\n",
    "words_in_fi={} #dictionary to story unique tokens in food-irrelevant class in training data\n",
    "words_in_col={}#dictionary to story unique tokens in both the classes in training data\n",
    "l = 0.7\n",
    "i=0\n",
    "col_len=0\n",
    "for line in data:\n",
    "    classs = line['label']\n",
    "    if (classs == 'Food-irrelevant'):\n",
    "        fi = fi + 1.0\n",
    "        text = line['text']  #extracting the text part from each review and copying into text string\n",
    "        text = text.lower()\n",
    "        words_list = re.findall('\\w+', text) #splitting it into each word\n",
    "        for word in words_list:\n",
    "            if word not in words_in_fi:\n",
    "                words_in_fi[word] = 1.0  #adding words into food_irrelevant class dictionary\n",
    "            else:\n",
    "                words_in_fi[word] = words_in_fi[word] + 1.0  #finding the frequency of each word in food_irrelevant class\n",
    "            if word not in words_in_col:\n",
    "                words_in_col[word] = 1.0 #adding words into all class dictionary\n",
    "            else:\n",
    "                words_in_col[word] = words_in_col[word] + 1.0 #finding the frequency of each word in both classes\n",
    "    else:\n",
    "        fr = fr + 1.0\n",
    "        text = line['text']\n",
    "        text = text.lower()\n",
    "        words_list = re.findall('\\w+', text)\n",
    "        for word in words_list:\n",
    "            if word not in words_in_fr:\n",
    "                words_in_fr[word] = 1.0 #adding words into food_relevant class dictionary\n",
    "            else:\n",
    "                words_in_fr[word] = words_in_fr[word] + 1.0 #finding the frequency of each word in food_relevant class\n",
    "            if word not in words_in_col:\n",
    "                words_in_col[word] = 1.0 #adding words into all class dictionary\n",
    "            else:\n",
    "                words_in_col[word] = words_in_col[word] + 1.0 #finding the frequency of each word in both classes\n",
    "#print \"size of words_in_fi is \",len(words_in_fi),\" size of words_in_fr is \",len(words_in_fr),\" size of words_in_col is \",len(words_in_col)\n",
    "for word in words_in_fi:\n",
    "    size_fi = size_fi + words_in_fi[word] #calculating size of fi class including repeated tokens\n",
    "for word in words_in_fr:\n",
    "    size_fr = size_fr + words_in_fr[word] #calculating size of fi class including repeated tokens\n",
    "for word in words_in_col:\n",
    "    col_len = col_len + words_in_col[word]\n",
    "\n",
    "#for word in words_in_fi:  #calculating probability of each term using mixture model\n",
    "#    words_in_fi[word] = l*((words_in_fi[word])/(size_fi)) + (1 - l)*((words_in_col[word])/(size_fi + size_fr))\n",
    "#for word in words_in_fr:  #calculating probability of each term using mixture model\n",
    "#    words_in_fr[word] = l*((words_in_fr[word])/(size_fr)) + (1 - l)*((words_in_col[word])/(size_fi + size_fr))\n",
    "for word in words_in_col:  #calculating probability of each term using mixture model\n",
    "    if word in words_in_fi:\n",
    "        words_in_fi[word] = l*((words_in_fi[word])/(size_fi)) + (1 - l)*((words_in_col[word])/(size_fi + size_fr))\n",
    "    else:\n",
    "        words_in_fi[word] = (1 - l)*((words_in_col[word])/(size_fi + size_fr))\n",
    "          \n",
    "    if word in words_in_fr:  #calculating probability of each term using mixture model\n",
    "        words_in_fr[word] = l*((words_in_fr[word])/(size_fr)) + (1 - l)*((words_in_col[word])/(size_fi + size_fr))    \n",
    "    else:\n",
    "        words_in_fr[word] = (1 - l)*((words_in_col[word])/(size_fi + size_fr))\n",
    "#print \"fi is \",size_fi,\" fr is \",size_fr,\" size is \",col_len,\" DISTINCT IS \",len(words_in_col),\" len of fi is \",len(words_in_fi),\" len of fr is \",len(words_in_fr)\n",
    "        \n",
    "#print \"fi is \",fi,\" fr is \",fr\n",
    "fi = fi + 0.0\n",
    "#print \"pr_fi is \",fi,\" pr_fr is \",fr\n",
    "fi = fi/(fi + fr)  #probability of fi class\n",
    "fr = 1.0 - fi #probability of fi class\n",
    "#print \"fi is \",fi,\" fr is \",fr\n",
    "#print \"pr_fi is \",fi,\" pr_fr is \",fr\n",
    "#print \"fi is \", size_fi,\" fr is \",size_fr,\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  89.0 %\n",
      "For Food-irrelevant class precision is  0.862068965517  recall is  0.78125\n",
      "For Food-relevant class precision is  0.901408450704  recall is  0.941176470588\n"
     ]
    }
   ],
   "source": [
    "# Apply your classifier on the test data. Report the results.\n",
    "# Insert as many cells as you want\n",
    "test_input = \"/home/karthik/Downloads/testing_data.json\"\n",
    "test_data=[]\n",
    "f = open(test_input,'r')\n",
    "for line in f:\n",
    "    test_data.append(json.loads(line))\n",
    "fo_i = 0\n",
    "fo_r = 0\n",
    "res_fi=0\n",
    "res_fr=0\n",
    "ii=0\n",
    "rr=0\n",
    "for line in test_data:\n",
    "    \n",
    "    test_text = line['text']   #extracting the text part from review\n",
    "    test_text = test_text.lower() #converting to lower case\n",
    "    test_words_list = re.split('\\W+', test_text)\n",
    "    test_words={}\n",
    "    pr_fi=math.log(fi)\n",
    "    pr_fr=math.log(fr)\n",
    "    for word in test_words_list:\n",
    "        if word not in test_words:\n",
    "               test_words[word] = 1.0\n",
    "        else:\n",
    "            test_words[word] = test_words[word] + 1.0\n",
    "    for word in test_words:\n",
    "        if word in words_in_fi:\n",
    "            pr_fi = pr_fi + math.log(words_in_fi[word]) * (test_words[word]) #recursively calculating probability for food-irrelevant class\n",
    "            #print \"pr_fi is \",pr_fi\n",
    "        if word in words_in_fr:\n",
    "            pr_fr = pr_fr + math.log(words_in_fr[word]) * (test_words[word]) #recursively calculating probability for food-relevant class\n",
    "    if (pr_fi>= pr_fr):\n",
    "        fo_i = fo_i + 1.0  #classifying as food-irrelevant\n",
    "        if (line['label'] == 'Food-irrelevant'):\n",
    "            ii = ii +1.0  #calculating true positives\n",
    "    else:\n",
    "        fo_r = fo_r + 1.0; #classifying as food-relevant\n",
    "        if (line['label'] == 'Food-relevant'):\n",
    "            rr = rr +1.0  #calculating true positives\n",
    "    if (line['label'] == 'Food-irrelevant'):\n",
    "        res_fi = res_fi + 1.0 #calculating actual no.of food-irrelevant\n",
    "    else:\n",
    "        res_fr = res_fr + 1.0 #calculating actual no.of food-relevant\n",
    "#print \"From my Classifier food irrelavant is \", fo_i ,\" \",ii,\" food relevant is \",fo_r,\" \",rr\n",
    "#print \"Correct numeber of food irrelavant is \", res_fi ,\" food relevant is \",res_fr\n",
    "accuracy = (ii + rr)/100\n",
    "precision_fi = ii/(fo_i)  #tp/(tp + fp)\n",
    "recall_fi = ii/(res_fi)   #tp/(tp + fn)\n",
    "precision_fr = rr/(fo_r)  #tp/(tp + fp)\n",
    "recall_fr = rr/(res_fr)   #tp/(tp + fn)\n",
    "\n",
    "print \"Accuracy is \", accuracy*100,\"%\"\n",
    "print \"For Food-irrelevant class precision is \",precision_fi,\" recall is \",recall_fi\n",
    "print \"For Food-relevant class precision is \",precision_fr,\" recall is \",recall_fr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Rocchio classifier [35 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, your job is to implement a Rocchio classifier for \"food-relevant vs. food-irrelevant\". You need to aggregate all the reviews of each class, and find the center. **Use the normalized raw term frequency**.\n",
    "\n",
    "\n",
    "### What to report\n",
    "\n",
    "* For the entire testing dataset, report the overall accuracy.\n",
    "* For the class \"Food-relevant\", report the precision and recall.\n",
    "* For the class \"Food-irrelevant\", report the precision and recall.\n",
    "\n",
    "We will also grade on the quality of your code. So make sure that your code is clear and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the Rocchio classifier\n",
    "# Insert as many cells as you want\n",
    "\n",
    "import os\n",
    "import re \n",
    "import json\n",
    "import math\n",
    "user_input = \"/home/karthik/Downloads/training_data.json\"\n",
    "data=[]    #data is copied from json file into this list\n",
    "f = open(user_input,'r')\n",
    "for line in f:\n",
    "    data.append(json.loads(line))\n",
    "fi=0     #contains the number of food-irrelevant reviews in training data\n",
    "fr=0     #contains the number of food-relevant reviews in training data\n",
    "cen_fi={} #centroid for food-irrelevant class\n",
    "cen_fr={} #centroid for food-relevant class\n",
    "l = 0.7\n",
    "for line in data:\n",
    "    classs = line['label']\n",
    "    words_in_fr={} #dictionary to story unique tokens in food-relevant class in training data for each review\n",
    "    words_in_fi={} #dictionary to story unique tokens in food-irrelevant class in training data for each review\n",
    "    length = 0.0\n",
    "    if (classs == 'Food-irrelevant'):\n",
    "        fi = fi + 1.0\n",
    "        text = line['text']  #extracting the text part from each review and copying into text string\n",
    "        text = text.lower()\n",
    "        words_list = re.findall(r\"[\\w]+\", text) #splitting it into each word\n",
    "        for word in words_list:\n",
    "            if word not in words_in_fi:\n",
    "                words_in_fi[word] = 1.0  #adding words into food_irrelevant class dictionary\n",
    "            else:\n",
    "                words_in_fi[word] = words_in_fi[word] + 1.0  #finding the frequency of each word in food_irrelevant class\n",
    "        for word in words_in_fi:  #magnitude of the vector is calculated\n",
    "            length = length + (words_in_fi[word] ** (2))  #numbers are squared\n",
    "        length = length **(0.5)  #square root is taken\n",
    "        for word in words_in_fi:\n",
    "            words_in_fi[word] = words_in_fi[word]/length  #normalization is done\n",
    "            if word in cen_fi:\n",
    "                cen_fi[word] = cen_fi[word] + words_in_fi[word] #centroid is found by adding all the vectors average is done later\n",
    "            else:\n",
    "                cen_fi[word] = words_in_fi[word]\n",
    "    else:  # the above same process is repeated for food-relavant class\n",
    "        fr = fr + 1.0\n",
    "        text = line['text']\n",
    "        text = text.lower()\n",
    "        words_list = re.findall(r\"[\\w]+\", text)\n",
    "        for word in words_list:\n",
    "            if word not in words_in_fr:\n",
    "                words_in_fr[word] = 1.0 #adding words into food_relevant class dictionary\n",
    "            else:\n",
    "                words_in_fr[word] = words_in_fr[word] + 1.0 #finding the frequency of each word in food_relevant class\n",
    "        for word in words_in_fr:\n",
    "            length = length + (words_in_fr[word] ** (2))\n",
    "        length = length **(0.5)\n",
    "        for word in words_in_fr:\n",
    "            words_in_fr[word] = words_in_fr[word]/length\n",
    "            if word in cen_fr:\n",
    "                cen_fr[word] = cen_fr[word] + words_in_fr[word]\n",
    "            else:\n",
    "                cen_fr[word] = words_in_fr[word]\n",
    "for word in cen_fi:\n",
    "    cen_fi[word] = cen_fi[word]/fi\n",
    "for word in cen_fr:\n",
    "    cen_fr[word] = cen_fr[word]/fr\n",
    "#print fi,fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  71.0 %\n",
      "For Food-irrelevant class precision is  0.540540540541  recall is  0.625\n",
      "For Food-relevant class precision is  0.809523809524  recall is  0.75\n"
     ]
    }
   ],
   "source": [
    "# Apply your classifier on the test data. Report the results.\n",
    "# Insert as many cells as you want\n",
    "\n",
    "test_input = \"/home/karthik/Downloads/testing_data.json\"\n",
    "test_data=[]\n",
    "f = open(test_input,'r')\n",
    "for line in f:\n",
    "    test_data.append(json.loads(line))\n",
    "fo_i = 0\n",
    "fo_r = 0\n",
    "res_fi=0\n",
    "res_fr=0\n",
    "ii=0\n",
    "rr=0\n",
    "for line in test_data:\n",
    "    length = 0.0\n",
    "    distance_fi = 0.0 #distance from food-irrelevant class\n",
    "    distance_fr = 0.0 #distance from food-relevant class\n",
    "    if (line['label'] == 'Food-irrelevant'):\n",
    "        res_fi = res_fi + 1\n",
    "    else:\n",
    "        res_fr = res_fr + 1\n",
    "    test_text = line['text']  #extracting the text part from review\n",
    "    test_text = test_text.lower() #converting to lower case\n",
    "    test_words_list = re.findall(r\"[\\w]+\", test_text)\n",
    "    test_words={}\n",
    "    for word in test_words_list:\n",
    "        if word not in test_words:\n",
    "               test_words[word] = 1.0\n",
    "        else:\n",
    "            test_words[word] = test_words[word] + 1.0\n",
    "    for word in test_words:\n",
    "        length = length + (test_words[word] ** 2)\n",
    "    length = length **(0.5)  #finding the magnitude of test vector\n",
    "    for word in test_words:\n",
    "        test_words[word] = test_words[word]/length #normalizing test vector\n",
    "        if word in cen_fi:\n",
    "            distance_fi = distance_fi + ((test_words[word] - cen_fi[word])**(2)) #finding distance by subtracting if word is there in both centroid and test vector\n",
    "        else:\n",
    "            distance_fi = distance_fi + ((test_words[word] - 0)**(2)) #if word not in centroid just taking 0 in its place\n",
    "        if word in cen_fr:\n",
    "            distance_fr = distance_fr + ((test_words[word] - cen_fr[word])**(2)) #finding distance by subtracting if word is there in both centroid and test vector\n",
    "        else:\n",
    "            distance_fr = distance_fr + ((test_words[word] - 0)**(2)) #if word not in centroid just taking 0 in its place\n",
    "    distance_fi = distance_fi ** (0.5)\n",
    "    distance_fr = distance_fr ** (0.5)\n",
    "    if (distance_fi >= distance_fr):\n",
    "        fo_r = fo_r + 1 #classifying as food-irrelevant\n",
    "        if (line['label'] == 'Food-relevant'):\n",
    "            rr = rr +1.0 #true positive\n",
    "    else:\n",
    "        fo_i = fo_i + 1 #classifying as food-relevant\n",
    "        if (line['label'] == 'Food-irrelevant'):\n",
    "            ii = ii +1.0 #true positive\n",
    "    \n",
    "#print \"food irrelavant is \", fo_i ,\" \",ii,\" food relevant is \",fo_r,rr\n",
    "#print \"food irrelavant is \", res_fi ,\" food relevant is \",res_fr\n",
    "\n",
    "precision_fi = ii/(fo_i) #tp/(tp + fp)\n",
    "recall_fi = ii/(res_fi)  #tp/(tp + fn)\n",
    "precision_fr = rr/(fo_r) #tp/(tp + fp)\n",
    "recall_fr = rr/(res_fr)  #tp/(tp + fn)\n",
    "\n",
    "print \"Accuracy is \",(ii + rr),\"%\"\n",
    "print \"For Food-irrelevant class precision is \",precision_fi,\" recall is \",recall_fi\n",
    "print \"For Food-relevant class precision is \",precision_fr,\" recall is \",recall_fr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Naive Bayes vs. Rocchio [20 points]\n",
    "\n",
    "Which method gives the better results? In terms of what? How did you compare them? Can you explain why you observe what you do? Write 1-3 paragraphs below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add your answer here:**\n",
    "\n",
    "Naive Bayes gave the best result. I used accuracy to compare these two. As we can say in Naive Bayes I was getting 89% accuracy where as in Rocchio I was getting 71% accuracy. Also the precision and recall is better for Naive Bayes compared to Rocchio. I think it is better because as the number of dimensions(words) increase then Naive Bayes gives a better result because of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Recommenders [10 points]\n",
    "\n",
    "Finally, since we've begun our discussion of recommenders, let's do a quick problem too:\n",
    "\n",
    "The table below is a utility matrix, representing the ratings, on a 1â€“5 star scale, of eight items, *a* through *h*, by three users *A*, *B*, and *C*. \n",
    "<pre>\n",
    "\n",
    "  | a  b  c  d  e  f  g  h\n",
    "--|-----------------------\n",
    "A | 4  5     5  1     3  2\n",
    "B |    3  4  3  1  2  1\n",
    "C | 2     1  3     4  5  3\n",
    "\n",
    "</pre>\n",
    "\n",
    "Compute the following from the data of this matrix.\n",
    "\n",
    "(a) Treating the utility matrix as boolean, compute the Jaccard distance between each pair of users.\n",
    "\n",
    "(b) Repeat Part (a), but use the cosine distance.\n",
    "\n",
    "(c) Treat ratings of 3, 4, and 5 as 1 and 1, 2, and blank as 0. Compute the Jaccard distance between each pair of users.\n",
    "\n",
    "(d) Repeat Part (c), but use the cosine distance.\n",
    "\n",
    "(e) Normalize the matrix by subtracting from each nonblank entry the average\n",
    "value for its user.\n",
    "\n",
    "(f) Using the normalized matrix from Part (e), compute the cosine distance\n",
    "between each pair of users.\n",
    "\n",
    "(g) Which of the approaches above seems most reasonable to you? Give a one or two sentence argument supporting your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add your answer here:**\n",
    "\n",
    "(a) J(A,B) = J(B,C) = J(C,A) = 0.5\n",
    "\n",
    "(b) Cos(A,B) = 0.6010 Cos(B,C) = 0.5139 Cos(C,A) = 0.6149  \n",
    "\n",
    "(c) J(A,B) = J(B,C) = J(C,A) = 0.5\n",
    "\n",
    "(d) Cos(A,B) = 0.5774 Cos(B,C) = 0.2887 Cos(C,A) = 0.5\n",
    "\n",
    "(e)    a    b    c     d    e     f      g     h\n",
    "   A 0.66  1.66       1.66 -2.33       -0.33  -1.33\n",
    "   B       0.66  1.66 0.66 -1.33 -0.33 -1.33\n",
    "   C -1          -2    0          1      2     0\n",
    "\n",
    "(f) Cos(A,B) = 0.5835 Cos(B,C) = -0.7399 Cos(C,A) = -0.1147\n",
    "\n",
    "(g) The normalized matrix approach seems most reasonable as a persons average can vary i.e he might give 3 as his least rating even when its not good and other person might give 1 as his least rating. So as this normalized approach takes care of it by subtracting the average of each user this is a better option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
